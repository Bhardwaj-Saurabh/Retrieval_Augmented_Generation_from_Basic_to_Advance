{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbw4wHV4zlKj"
   },
   "source": [
    "# Build a Contextual Retrieval based RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn how to build a Contextual Retrieval-Augmented Generation (RAG) system using LangChain, OpenAI, and Chroma vector database. The workflow covers the entire RAG pipeline, including:\n",
    "\n",
    "- Loading and processing structured (JSON) and unstructured (PDF) documents.\n",
    "- Chunking documents and generating contextual summaries for each chunk to improve retrieval quality.\n",
    "- Creating embeddings for all document chunks using OpenAI's embedding models.\n",
    "- Indexing the embeddings in a persistent Chroma vector database for efficient semantic search.\n",
    "- Implementing a semantic retriever to fetch relevant document chunks based on user queries.\n",
    "- Building a RAG pipeline that uses retrieved context to answer questions with a language model.\n",
    "- Extending the pipeline to provide source citations for generated answers, including quoted evidence from the original documents.\n",
    "- Displaying results with highlighted citations and context for transparency and traceability.\n",
    "\n",
    "It is intended as a practical guide for building advanced RAG systems with source attribution, suitable for research, knowledge management, and production AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1734093612694,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "Jhfb4gMUfvAC"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# set OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7059,
     "status": "ok",
     "timestamp": 1734093622010,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "-On4AS0HfvAD"
   },
   "outputs": [],
   "source": [
    "# initialize OpenAI embeddings model\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afzeN_WkHIz2"
   },
   "source": [
    "## Loading and Processing the Data\n",
    "\n",
    "Follow notebook in lession 3 for more detail explaination on basic RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ5y0NfzHPhg"
   },
   "outputs": [],
   "source": [
    "# load wiki data JSONL file\n",
    "loader = JSONLoader(file_path='../../rag_docs/wikidata_rag_demo.jsonl',\n",
    "                    jq_schema='.',\n",
    "                    text_content=False,\n",
    "                    json_lines=True)\n",
    "wiki_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yICyAF85h2DO"
   },
   "outputs": [],
   "source": [
    "# process wiki documents\n",
    "wiki_docs_processed = []\n",
    "\n",
    "for doc in wiki_docs:\n",
    "    doc = json.loads(doc.page_content)\n",
    "    metadata = {\n",
    "        \"title\": doc['title'],\n",
    "        \"id\": doc['id'],\n",
    "        \"source\": \"Wikipedia\",\n",
    "        \"page\": 1\n",
    "    }\n",
    "    data = ' '.join(doc['paragraphs'])\n",
    "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_GzvHP1jSBo"
   },
   "source": [
    "### Load and Process PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vH6xGFOnv7m"
   },
   "source": [
    "#### Create Chunk Contexts for Contextual Retrieval\n",
    "\n",
    "![CR](../../images/contextual_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxHHyhlbl_9j"
   },
   "outputs": [],
   "source": [
    "# initialize ChatOpenAI model\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHSh0Vg-mIUv"
   },
   "outputs": [],
   "source": [
    "# create chunk context generation chain\n",
    "def generate_chunk_context(document, chunk):\n",
    "\n",
    "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
    "                            Your task is to provide brief, relevant context for a chunk of text\n",
    "                            based on the following research paper.\n",
    "\n",
    "                            Here is the research paper:\n",
    "                            <paper>\n",
    "                            {paper}\n",
    "                            </paper>\n",
    "\n",
    "                            Here is the chunk we want to situate within the whole document:\n",
    "                            <chunk>\n",
    "                            {chunk}\n",
    "                            </chunk>\n",
    "\n",
    "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
    "                            considering the following guidelines:\n",
    "\n",
    "                            - Give a short succinct context to situate this chunk within the overall document\n",
    "                            for the purposes of improving search retrieval of the chunk.\n",
    "                            - Answer only with the succinct context and nothing else.\n",
    "                            - Context should be mentioned like 'Focuses on ....'\n",
    "                            do not mention 'this chunk or section focuses on...'\n",
    "\n",
    "                            Context:\n",
    "                        \"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
    "\n",
    "    agentic_chunk_chain = (prompt_template\n",
    "                                |\n",
    "                            chatgpt\n",
    "                                |\n",
    "                            StrOutputParser())\n",
    "\n",
    "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uxOSfcsxqHD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create contextual chunks from PDF document\n",
    "def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
    "\n",
    "    print('Loading pages:', file_path)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "\n",
    "    print('Chunking pages:', file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                              chunk_overlap=chunk_overlap)\n",
    "    doc_chunks = splitter.split_documents(doc_pages)\n",
    "\n",
    "    print('Generating contextual chunks:', file_path)\n",
    "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
    "    contextual_chunks = []\n",
    "    for chunk in doc_chunks:\n",
    "        chunk_content = chunk.page_content\n",
    "        chunk_metadata = chunk.metadata\n",
    "        chunk_metadata_upd = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'page': chunk_metadata['page'],\n",
    "            'source': chunk_metadata['source'],\n",
    "            'title': chunk_metadata['source'].split('/')[-1]\n",
    "        }\n",
    "        context = generate_chunk_context(original_doc, chunk_content)\n",
    "        contextual_chunks.append(Document(page_content=context+'\\n'+chunk_content,\n",
    "                                          metadata=chunk_metadata_upd))\n",
    "        time.sleep(10)  # to avoid rate limits\n",
    "    print('Finished processing:', file_path)\n",
    "    print()\n",
    "    return contextual_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-VUpdmczt0C",
    "outputId": "efe8db0c-6e47-464e-926f-2c993314d190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../rag_docs/cnn_paper.pdf',\n",
       " '../../rag_docs/vision_transformer.pdf',\n",
       " '../../rag_docs/resnet_paper.pdf',\n",
       " '../../rag_docs/attention_paper.pdf']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of PDF files\n",
    "pdf_files = glob('../../rag_docs/*.pdf')\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsicBMPazzlg",
    "outputId": "8b567e07-d403-4ae6-d13b-98d02b0a24b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages: ../../rag_docs/cnn_paper.pdf\n",
      "Chunking pages: ../../rag_docs/cnn_paper.pdf\n",
      "Generating contextual chunks: ../../rag_docs/cnn_paper.pdf\n",
      "Finished processing: ../../rag_docs/cnn_paper.pdf\n",
      "\n",
      "Loading pages: ../../rag_docs/vision_transformer.pdf\n",
      "Chunking pages: ../../rag_docs/vision_transformer.pdf\n",
      "Generating contextual chunks: ../../rag_docs/vision_transformer.pdf\n",
      "Finished processing: ../../rag_docs/vision_transformer.pdf\n",
      "\n",
      "Loading pages: ../../rag_docs/resnet_paper.pdf\n",
      "Chunking pages: ../../rag_docs/resnet_paper.pdf\n",
      "Generating contextual chunks: ../../rag_docs/resnet_paper.pdf\n",
      "Finished processing: ../../rag_docs/resnet_paper.pdf\n",
      "\n",
      "Loading pages: ../../rag_docs/attention_paper.pdf\n",
      "Chunking pages: ../../rag_docs/attention_paper.pdf\n",
      "Generating contextual chunks: ../../rag_docs/attention_paper.pdf\n",
      "Finished processing: ../../rag_docs/attention_paper.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process PDF documents to create contextual chunks\n",
    "paper_docs = []\n",
    "for fp in pdf_files:\n",
    "    paper_docs.extend(create_contextual_chunks(file_path=fp, chunk_size=3500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of machine learning, highlighting their significance as advanced architectures of Artificial Neural Networks (ANNs) specifically designed for image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs and sets the stage for a deeper exploration of CNNs and their applications in image analysis.\n",
      "An Introduction to Convolutional Neural Networks\n",
      "Keiron O’Shea1 and Ryan Nash2\n",
      "1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\n",
      "keo7@aber.ac.uk\n",
      "2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n",
      "4YW\n",
      "nashrd@live.lancs.ac.uk\n",
      "Abstract. The ﬁeld of machine learning has taken a dramatic twist in re-\n",
      "cent times, with the rise of the Artiﬁcial Neural Network (ANN). These\n",
      "biologically inspired computational models are able to far exceed the per-\n",
      "formance of previous forms of artiﬁcial intelligence in common machine\n",
      "learning tasks. One of the most impressive forms of ANN architecture is\n",
      "that of the Convolutional Neural Network (CNN). CNNs are primarily\n",
      "used to solve difﬁcult image-driven pattern recognition tasks and with\n",
      "their precise yet simple architecture, offers a simpliﬁed method of getting\n",
      "started with ANNs.\n",
      "This document provides a brief introduction to CNNs, discussing recently\n",
      "published papers and newly formed techniques in developing these bril-\n",
      "liantly fantastic image recognition models. This introduction assumes you\n",
      "are familiar with the fundamentals of ANNs and machine learning.\n",
      "Keywords: Pattern recognition, artiﬁcial neural networks, machine learn-\n",
      "ing, image analysis\n",
      "1\n",
      "Introduction\n",
      "Artiﬁcial Neural Networks (ANNs) are computational processing systems of\n",
      "which are heavily inspired by way biological nervous systems (such as the hu-\n",
      "man brain) operate. ANNs are mainly comprised of a high number of intercon-\n",
      "nected computational nodes (referred to as neurons), of which work entwine in\n",
      "a distributed fashion to collectively learn from the input in order to optimise its\n",
      "ﬁnal output.\n",
      "The basic structure of a ANN can be modelled as shown in Figure 1. We would\n",
      "load the input, usually in the form of a multidimensional vector to the input\n",
      "layer of which will distribute it to the hidden layers. The hidden layers will then\n",
      "make decisions from the previous layer and weigh up how a stochastic change\n",
      "within itself detriments or improves the ﬁnal output, and this is referred to as\n",
      "the process of learning. Having multiple hidden layers stacked upon each-other\n",
      "is commonly called deep learning.\n",
      "arXiv:1511.08458v2  [cs.NE]  2 Dec 2015\n"
     ]
    }
   ],
   "source": [
    "# display first contextual chunk\n",
    "print(paper_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyPdlZo2xEly"
   },
   "source": [
    "### Combine all document chunks in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNQWgq9t0pMH",
    "outputId": "58ca4c0c-18b4-4fbc-a77c-800e835a3610"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1880"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_docs = wiki_docs_processed + paper_docs\n",
    "len(total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Daqn6Hglw9Nk"
   },
   "source": [
    "## Index Document Chunks and Embeddings in Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhAQyrFBfvAN"
   },
   "outputs": [],
   "source": [
    "# Index Document Chunks and Embeddings in Vector DB\n",
    "chroma_db = Chroma.from_documents(documents=total_docs,\n",
    "                                  collection_name='context_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./context_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ju_zBIj1Zsb"
   },
   "source": [
    "### Load Vector DB from disk\n",
    "\n",
    "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pNvj0dDH1WDg"
   },
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"./context_db\",\n",
    "                   collection_name='context_db',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njfZOOVZxj1a"
   },
   "source": [
    "### Semantic Similarity based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV1l6HYdxj1b"
   },
   "outputs": [],
   "source": [
    "# Semantic Similarity based Retrieval\n",
    "retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUIJG_bDxj1c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# function to display documents\n",
    "def display_docs(docs):\n",
    "    for doc in docs:\n",
    "        print('Metadata:', doc.metadata)\n",
    "        print('Content Brief:')\n",
    "        display(Markdown(doc.page_content[:1000]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test retriever with basic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "PIh4xGv2xj1c",
    "outputId": "f8b431ff-094d-49ab-9bcd-e2c9aae2a632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'title': 'Machine learning', 'id': '564928', 'source': 'Wikipedia', 'page': 1}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'source': 'Wikipedia', 'page': 1, 'id': '359370', 'title': 'Supervised learning'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '663523', 'source': 'Wikipedia', 'page': 1, 'title': 'Deep learning'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'source': 'Wikipedia', 'title': 'Artificial intelligence', 'page': 1, 'id': '6360'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '44742', 'title': 'Artificial neural network', 'page': 1, 'source': 'Wikipedia'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is machine learning?\"\n",
    "top_docs = retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S_PXFMcJxuyO",
    "outputId": "5133bece-a361-4543-9214-1c80fe673353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': '../../rag_docs/vision_transformer.pdf', 'id': '0ea6821d-e794-4ca0-9dee-0f9ee8a499cc', 'title': 'vision_transformer.pdf', 'page': 7}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability. Additionally, it discusses the implications of hybrid models and the potential for further scaling of Vision Transformers.\n",
       "Published as a conference paper at ICLR 2021\n",
       "4.4\n",
       "SCALING STUDY\n",
       "We perform a controlled scaling study of different models by evaluating transfer performance from\n",
       "JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\n",
       "performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\n",
       "R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\n",
       "for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\n",
       "L/16 and H/14"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'source': '../../rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf', 'id': '67372578-19cd-48d4-9767-196e5c15e471', 'page': 0}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that a pure Transformer can achieve competitive performance on various image recognition benchmarks when pre-trained on large datasets.\n",
       "Published as a conference paper at ICLR 2021\n",
       "AN IMAGE IS WORTH 16X16 WORDS:\n",
       "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
       "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\n",
       "Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\n",
       "Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n",
       "∗equal technical contribution, †equal advising\n",
       "Google Research, Brain Team\n",
       "{adosovitskiy, neilhoulsby}@google.com\n",
       "ABSTRACT\n",
       "While the Transformer architecture has become the de-facto standard for natural\n",
       "language pr"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'source': '../../rag_docs/vision_transformer.pdf', 'page': 2, 'id': '17fc9c5f-3529-4522-89df-abf28bfb5590', 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\n",
       "Published as a conference paper at ICLR 2021\n",
       "Transformer Encoder\n",
       "MLP \n",
       "Head\n",
       "Vision Transformer (ViT)\n",
       "*\n",
       "Linear Projection of Flattened Patches\n",
       "* Extra learnable\n",
       "     [ cl ass]  embedding\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "0\n",
       "Patch + Position \n",
       "Embedding\n",
       "Class\n",
       "Bird\n",
       "Ball\n",
       "Car\n",
       "...\n",
       "Embedded \n",
       "Patches\n",
       "Multi-Head \n",
       "Attention\n",
       "Norm\n",
       "MLP\n",
       "Norm\n",
       "+\n",
       "L x\n",
       "+\n",
       "Transformer Encoder\n",
       "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\n",
       "add position embeddings, and feed the resulting sequence of vectors to a standard Transformer\n",
       "encoder. In order to perform classiﬁ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'source': '../../rag_docs/vision_transformer.pdf', 'id': '6d2d3b62-4961-479f-96fe-1ed226d74dda', 'page': 1, 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the performance of the Vision Transformer (ViT) when trained on large datasets, highlighting its ability to achieve state-of-the-art results in image recognition tasks despite lacking some inductive biases inherent to convolutional neural networks (CNNs). It also discusses related work in the field, particularly the application of self-attention mechanisms in image processing and comparisons with existing models.\n",
       "Published as a conference paper at ICLR 2021\n",
       "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\n",
       "when trained on insufﬁcient amounts of data.\n",
       "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
       "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
       "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
       "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT ap"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'page': 7, 'title': 'vision_transformer.pdf', 'id': 'def024b1-1080-4288-bfaf-48e07c7981da', 'source': '../../rag_docs/vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the behavior of attention mechanisms in Vision Transformers, highlighting how attention distances vary across layers and the implications of localized attention in hybrid models that incorporate CNNs. It also discusses the relationship between attention distance and network depth, indicating that deeper layers attend to semantically relevant regions for classification.\n",
       "have consistently small attention distances in the low layers. This highly localized attention is\n",
       "less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\n",
       "suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\n",
       "attention distance increases with network depth. Globally, we ﬁnd that the model attends to image\n",
       "regions that are semantically relevant for classiﬁcation (Figure 6).\n",
       "4.6\n",
       "SELF-SUPERVISION\n",
       "Transformers show impressive performance on NLP tasks. However, much of their success stems\n",
       "not only from their excellent scalability bu"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the difference between transformers and vision transformers?\"\n",
    "top_docs = retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQFWv7YUyVII"
   },
   "source": [
    "## Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHOrfGXKyVIJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# RAG Prompt Template\n",
    "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
    "                Answer the following question using only the following pieces of retrieved context.\n",
    "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
    "                Keep the answer detailed and well formatted based on the information from the context.\n",
    "\n",
    "                Question:\n",
    "                {question}\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmWeCB4yyVIJ"
   },
   "outputs": [],
   "source": [
    "# initialize ChatOpenAI model\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG Chain\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    rag_prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "xvj_eGIWyVIJ",
    "outputId": "1320207b-c57a-4e08-f755-0f050045884b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n",
       "\n",
       "Machine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Some common applications of machine learning include:\n",
       "\n",
       "- Spam filtering\n",
       "- Detection of network intruders or malicious insiders\n",
       "- Optical character recognition (OCR)\n",
       "- Search engines\n",
       "- Computer vision\n",
       "\n",
       "Within machine learning, there are different types of learning approaches, such as supervised learning, where a function is inferred from labeled training data. In supervised learning, the system learns to produce correct results based on known outcomes, typically using vectors for training data and results.\n",
       "\n",
       "Additionally, deep learning is a specialized area of machine learning that utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). Deep learning is effective for complex tasks like speech recognition, image understanding, and handwriting recognition, which are challenging for computers but relatively easy for humans. \n",
       "\n",
       "Overall, machine learning represents a significant advancement in the ability of computers to process information and make decisions autonomously, drawing inspiration from biological systems while employing distinct methodologies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test RAG Chain with basic questions\n",
    "query = \"What is machine learning?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 750
    },
    "id": "pXtezDlZzadt",
    "outputId": "747fbcca-5fcc-40f4-d903-2d70b334444d"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A CNN, or Convolutional Neural Network, is a specialized type of Artificial Neural Network (ANN) that is particularly effective for image-driven pattern recognition tasks. CNNs are designed to process data with a grid-like topology, such as images, and they utilize a unique architecture that distinguishes them from traditional ANNs.\n",
       "\n",
       "### Key Features of CNNs:\n",
       "\n",
       "1. **Three-Dimensional Neuron Organization**: \n",
       "   - The neurons in CNNs are organized into three dimensions: height, width, and depth. This structure allows CNNs to capture spatial hierarchies in images.\n",
       "\n",
       "2. **Layer Types**:\n",
       "   - CNNs consist of several types of layers:\n",
       "     - **Convolutional Layers**: These layers apply convolution operations to the input, allowing the network to learn spatial hierarchies of features. Each neuron in a convolutional layer is connected to a small region of the input, which helps in detecting local patterns.\n",
       "     - **Pooling Layers**: These layers reduce the spatial dimensions of the input, helping to decrease the computational load and control overfitting.\n",
       "     - **Fully-Connected Layers**: These layers connect every neuron in one layer to every neuron in the next layer, typically used at the end of the network to produce the final output.\n",
       "\n",
       "3. **Architecture**:\n",
       "   - A typical CNN architecture may involve stacking multiple convolutional layers followed by pooling layers. This stacking allows the network to learn increasingly complex features of the input data.\n",
       "\n",
       "4. **Learning Paradigms**:\n",
       "   - CNNs primarily utilize supervised learning, where the model is trained on labeled data to minimize classification errors. This is crucial for tasks such as image classification.\n",
       "\n",
       "5. **Applications**:\n",
       "   - CNNs are widely used in various applications, including image classification, object detection, and image segmentation, due to their ability to effectively encode image-specific features.\n",
       "\n",
       "In summary, CNNs are a powerful class of neural networks that excel in image analysis and pattern recognition, leveraging their unique architecture to process and learn from visual data efficiently."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is a CNN?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "Fo92-ZmIELPF",
    "outputId": "1c1cc31c-8ea4-4310-9975-8cee615f3f0e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A ResNet (Residual Network) is considered better than a traditional CNN (Convolutional Neural Network) for several reasons, primarily due to its architectural innovations that address optimization challenges associated with deeper networks. Here are the key advantages of ResNets over standard CNNs:\n",
       "\n",
       "1. **Residual Learning Framework**:\n",
       "   - ResNets introduce the concept of residual learning, where the network learns to fit a residual mapping instead of the original unreferenced mapping. This is formalized as \\( F(x) = H(x) - x \\), where \\( H(x) \\) is the desired mapping. The output is then expressed as \\( F(x) + x \\). This approach simplifies the optimization process, making it easier for the network to learn.\n",
       "\n",
       "2. **Shortcut Connections**:\n",
       "   - ResNets utilize shortcut connections that skip one or more layers, allowing the gradient to flow more easily during backpropagation. This helps mitigate the vanishing gradient problem, which is common in very deep networks. The identity mappings introduced by these shortcuts do not add extra parameters or computational complexity, yet they significantly enhance the training dynamics.\n",
       "\n",
       "3. **Performance with Increased Depth**:\n",
       "   - Traditional CNNs often suffer from increased training error as the depth of the network increases, a phenomenon known as the degradation problem. In contrast, ResNets can maintain or even improve performance as they become deeper. For instance, a 34-layer ResNet outperforms an 18-layer ResNet by a notable margin, demonstrating that deeper networks can be effectively trained without degradation in performance.\n",
       "\n",
       "4. **Generalization and Accuracy**:\n",
       "   - ResNets have shown superior generalization performance across various tasks, including image classification and object detection. They have achieved state-of-the-art results on benchmarks like ImageNet, with deeper architectures (e.g., 101-layer and 152-layer ResNets) yielding significant accuracy gains compared to shallower models. For example, a 152-layer ResNet achieved a top-5 validation error of 4.49%, outperforming previous models.\n",
       "\n",
       "5. **Empirical Evidence**:\n",
       "   - Extensive experiments on datasets like ImageNet and CIFAR-10 have demonstrated that ResNets not only converge faster but also achieve lower training and validation errors compared to their plain counterparts. This is evidenced by the fact that ResNets can be trained with over 100 layers while maintaining lower complexity than traditional networks like VGG.\n",
       "\n",
       "In summary, ResNets improve upon traditional CNNs by addressing optimization difficulties through residual learning and shortcut connections, allowing for deeper architectures that are easier to train and achieve better performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How is a resnet better than a CNN?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "J5IQoBc0zlAr",
    "outputId": "4ce7096b-f5b8-442b-b9c5-00bbe5e7a360"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on enabling computers to automatically understand and generate human languages. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages. The overarching goal of NLP is to facilitate seamless interaction between humans and machines through language.\n",
       "\n",
       "NLP is closely related to linguistics, which is the scientific study of language and its structure. Linguistics provides the foundational theories and frameworks that inform the development of NLP technologies. By leveraging insights from linguistics, NLP aims to enhance the ability of computers to process and analyze human language in a way that is meaningful and contextually appropriate. This relationship underscores the importance of understanding language structure, semantics, and syntax in the design of effective NLP systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is NLP and its relation to linguistics?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "AzeZuG1hzvGy",
    "outputId": "ca55a840-f295-413f-d01a-54bc383e6b4b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The difference between AI, ML, and DL can be summarized as follows:\n",
       "\n",
       "### Artificial Intelligence (AI)\n",
       "- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications aimed at making machines \"smart.\"\n",
       "- **Functionality**: AI systems can interpret external data, learn from it, and use those learnings to achieve specific goals through flexible adaptation. The term has evolved, and tasks once considered AI, like optical character recognition, are now seen as routine technologies.\n",
       "\n",
       "### Machine Learning (ML)\n",
       "- **Definition**: ML is a subfield of AI that focuses on the study and construction of algorithms that allow computers to learn from data without being explicitly programmed. It emerged from the broader field of artificial intelligence.\n",
       "- **Functionality**: ML algorithms build models from sample inputs and can make predictions or decisions based on data. It is particularly useful in scenarios where designing explicit algorithms is impractical, such as spam filtering and computer vision.\n",
       "\n",
       "### Deep Learning (DL)\n",
       "- **Definition**: DL is a specialized subset of machine learning that primarily uses neural networks with multiple layers (multi-layer neural networks) to process data. It is often referred to as deep structured learning or hierarchical learning.\n",
       "- **Functionality**: In deep learning, the information processed becomes more abstract with each added layer, making it particularly effective for complex tasks like speech and image recognition. DL models are inspired by the information processing patterns of biological nervous systems.\n",
       "\n",
       "In summary, AI is the overarching field that includes both ML and DL. ML is a specific approach within AI that focuses on learning from data, while DL is a further specialization of ML that utilizes deep neural networks to handle complex data representations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is the difference between AI, ML and DL?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "cWihDiL3zPzY",
    "outputId": "b8b3a24f-b11c-424d-efcd-6d5530148d15"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The difference between transformers and vision transformers primarily lies in their application and the way they process input data.\n",
       "\n",
       "1. **Transformers**:\n",
       "   - Originally designed for natural language processing (NLP) tasks, transformers utilize self-attention mechanisms to process sequences of tokens (words).\n",
       "   - In NLP, the input is typically a sequence of words, and the transformer architecture processes these tokens to understand context and relationships between them.\n",
       "   - Transformers have become the standard model for various NLP tasks due to their efficiency and scalability, allowing for the training of very large models.\n",
       "\n",
       "2. **Vision Transformers (ViT)**:\n",
       "   - Vision Transformers adapt the transformer architecture for image classification tasks by treating image patches as tokens.\n",
       "   - Instead of processing sequences of words, ViTs split an image into fixed-size patches, flatten these patches, and then embed them into a sequence that is fed into a standard transformer encoder.\n",
       "   - ViTs incorporate position embeddings to retain spatial information about the patches, which is crucial for understanding the structure of images.\n",
       "   - The architecture allows for global integration of information across the entire image, leveraging the self-attention mechanism to capture relationships between different parts of the image.\n",
       "\n",
       "In summary, while both transformers and vision transformers utilize the same underlying architecture, their applications differ significantly: transformers are used for sequential data (like text), whereas vision transformers are specifically designed to handle image data by treating image patches as sequences of tokens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is the difference between transformers and vision transformers?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "k1lqzejlEvsj",
    "outputId": "a21b4676-1ccb-477a-b2c6-4b6bda910171"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention is crucial in transformers for several reasons, particularly in the context of Vision Transformers (ViTs) and their application in image processing. Here are the key points regarding the importance of self-attention in transformers:\n",
       "\n",
       "1. **Localized Attention**: In Vision Transformers, self-attention mechanisms exhibit highly localized attention in the lower layers. This means that these layers focus on small, specific regions of the input image, similar to the early convolutional layers in Convolutional Neural Networks (CNNs). This localized attention is essential for capturing fine-grained details in images.\n",
       "\n",
       "2. **Attention Distance and Network Depth**: The attention distance, which refers to how far the model can attend to different parts of the input, increases with the depth of the network. In deeper layers, the model is able to attend to semantically relevant regions of the image that are important for classification tasks. This hierarchical attention allows the model to build a more comprehensive understanding of the image as it processes it through multiple layers.\n",
       "\n",
       "3. **Scalability and Efficiency**: Traditional self-attention mechanisms can be computationally expensive, especially when applied to images where each pixel could potentially attend to every other pixel. However, transformers have adapted self-attention to work efficiently with images by using approximations, such as applying attention only within local neighborhoods or using sparse attention mechanisms. This adaptation allows transformers to scale effectively to larger input sizes without losing the benefits of self-attention.\n",
       "\n",
       "4. **Performance in Image Recognition**: The application of self-attention in transformers has led to impressive performance in image recognition tasks. For instance, the Vision Transformer (ViT) has achieved state-of-the-art results on various benchmarks when trained on large datasets, demonstrating that self-attention can effectively replace some of the inductive biases inherent to CNNs.\n",
       "\n",
       "5. **Flexibility in Architecture**: Self-attention mechanisms allow for flexibility in how features are processed. They can be combined with CNNs or used independently, enabling a variety of architectures that can be tailored to specific tasks in computer vision.\n",
       "\n",
       "In summary, self-attention is vital in transformers as it enables localized attention, scales efficiently, enhances performance in image recognition, and provides architectural flexibility, all of which contribute to the model's ability to understand and classify images effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How is self-attention important in transformers?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "7Zf_BjmlFBcb",
    "outputId": "97aa2758-d215-4fe3-8f5a-a4fa2a1b56c2"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A ResNet, or Residual Network, operates on the principle of residual learning to address the challenges associated with training deep neural networks. Here’s a detailed explanation of how it works:\n",
       "\n",
       "### Key Concepts of ResNet\n",
       "\n",
       "1. **Residual Learning Framework**:\n",
       "   - Instead of directly learning the desired underlying mapping \\( H(x) \\), ResNets learn a residual mapping \\( F(x) = H(x) - x \\). This means that the network is trained to predict the difference between the output and the input, which is often easier than learning the output directly.\n",
       "\n",
       "2. **Shortcut Connections**:\n",
       "   - ResNets utilize shortcut connections that skip one or more layers. These connections perform identity mapping, allowing the input \\( x \\) to be added directly to the output of the stacked layers. This can be mathematically represented as:\n",
       "     \\[\n",
       "     H(x) = F(x) + x\n",
       "     \\]\n",
       "   - The addition of these shortcuts does not introduce extra parameters or computational complexity, making it efficient to train.\n",
       "\n",
       "3. **Optimization Benefits**:\n",
       "   - The architecture allows for easier optimization of deeper networks. Traditional deep networks (plain networks) often suffer from increased training errors as depth increases, a phenomenon known as the degradation problem. In contrast, ResNets can maintain or improve accuracy as they become deeper, effectively overcoming this issue.\n",
       "\n",
       "4. **Empirical Evidence**:\n",
       "   - Experiments have shown that ResNets can achieve significantly lower training errors compared to their plain counterparts, even as the number of layers increases. For instance, a 34-layer ResNet outperforms an 18-layer ResNet, demonstrating that deeper networks can be more effective when using residual learning.\n",
       "\n",
       "5. **Architectural Efficiency**:\n",
       "   - ResNets can be constructed with various depths (e.g., 18, 34, 50, 101, and 152 layers) while maintaining lower complexity compared to other architectures like VGG. This efficiency allows for the construction of very deep networks without the typical drawbacks associated with depth.\n",
       "\n",
       "### Conclusion\n",
       "In summary, ResNets leverage the concept of residual learning through shortcut connections to facilitate the training of very deep networks. This approach not only simplifies the optimization process but also leads to improved performance in various image recognition tasks, as evidenced by their success in competitions like ILSVRC 2015."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How does a resnet work?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "UbaUpVXKz8IK",
    "outputId": "7a4dc86a-91e4-458b-97bc-91b0595d7009"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is LangGraph?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "FmxNo6B50AdP",
    "outputId": "25797c69-e76e-4a30-cab3-334e0fd18178"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is an Agentic AI System?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "Ttz53mEy0J_D",
    "outputId": "da8bcee5-d16e-4638-9ed8-06987aba9ccf"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is LangChain?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25Si_mSAc9HY"
   },
   "source": [
    "# Build a RAG System with Source Citations Agentic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3cKwTUIc9HZ",
    "outputId": "2ca7b70c-cae1-4e73-c20a-653354a1c315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant who is an expert in question-answering tasks.\n",
      "                Answer the following question using only the following pieces of retrieved context.\n",
      "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
      "                Keep the answer detailed and well formatted based on the information from the context.\n",
      "\n",
      "                Question:\n",
      "                \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "\n",
      "                Context:\n",
      "                \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "                Answer:\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
    "                Answer the following question using only the following pieces of retrieved context.\n",
    "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
    "                Keep the answer detailed and well formatted based on the information from the context.\n",
    "\n",
    "                Question:\n",
    "                {question}\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n",
    "rag_prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fbhOllRprRx",
    "outputId": "26e90a30-253c-4220-adf7-20269e9dc4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant who is an expert in analyzing answers to questions\n",
      "                      and finding out referenced citations from context articles.\n",
      "\n",
      "                      Given the following question, context and generated answer,\n",
      "                      analyze the generated answer and quote citations from context articles\n",
      "                      that can be used to justify the generated answer.\n",
      "\n",
      "                      Question:\n",
      "                      \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "\n",
      "                      Context Articles:\n",
      "                      \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "                      Answer:\n",
      "                      \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "                  \n"
     ]
    }
   ],
   "source": [
    "citations_prompt = \"\"\"You are an assistant who is an expert in analyzing answers to questions\n",
    "                      and finding out referenced citations from context articles.\n",
    "\n",
    "                      Given the following question, context and generated answer,\n",
    "                      analyze the generated answer and quote citations from context articles\n",
    "                      that can be used to justify the generated answer.\n",
    "\n",
    "                      Question:\n",
    "                      {question}\n",
    "\n",
    "                      Context Articles:\n",
    "                      {context}\n",
    "\n",
    "                      Answer:\n",
    "                      {answer}\n",
    "                  \"\"\"\n",
    "\n",
    "cite_prompt_template = ChatPromptTemplate.from_template(citations_prompt)\n",
    "cite_prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-ehwnM4dyGV"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# define Citation and QuotedCitations models\n",
    "class Citation(BaseModel):\n",
    "    id: str = Field(description=\"\"\"The string ID of a SPECIFIC context article\n",
    "                                   which justifies the answer.\"\"\")\n",
    "    source: str = Field(description=\"\"\"The source of the SPECIFIC context article\n",
    "                                       which justifies the answer.\"\"\")\n",
    "    title: str = Field(description=\"\"\"The title of the SPECIFIC context article\n",
    "                                      which justifies the answer.\"\"\")\n",
    "    page: int = Field(description=\"\"\"The page number of the SPECIFIC context article\n",
    "                                     which justifies the answer.\"\"\")\n",
    "    quotes: str = Field(description=\"\"\"The VERBATIM sentences from the SPECIFIC context article\n",
    "                                      that are used to generate the answer.\n",
    "                                      Should be exact sentences from context article without missing words.\"\"\")\n",
    "\n",
    "# define QuotedCitations model\n",
    "class QuotedCitations(BaseModel):\n",
    "    \"\"\"Quote citations from given context articles\n",
    "       that can be used to justify the generated answer. Can be multiple articles.\"\"\"\n",
    "    citations: List[Citation] = Field(description=\"\"\"Citations (can be multiple) from the given\n",
    "                                                     context articles that justify the answer.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haxp_9LmjVp1"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "# initialize ChatOpenAI model with structured output\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "structured_chatgpt = chatgpt.with_structured_output(QuotedCitations)\n",
    "\n",
    "# function to format documents with metadata\n",
    "def format_docs_with_metadata(docs: List[Document]) -> str:\n",
    "    formatted_docs = [\n",
    "        f\"\"\"Context Article ID: {doc.metadata['id']}\n",
    "            Context Article Source: {doc.metadata['source']}\n",
    "            Context Article Title: {doc.metadata['title']}\n",
    "            Context Article Page: {doc.metadata['page']}\n",
    "            Context Article Details: {doc.page_content}\n",
    "         \"\"\"\n",
    "            for i, doc in enumerate(docs)\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# RAG with Citations Chains\n",
    "rag_response_chain = (\n",
    "    {\n",
    "        \"context\": (itemgetter('context')\n",
    "                        |\n",
    "                    RunnableLambda(format_docs_with_metadata)),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "        |\n",
    "    rag_prompt_template\n",
    "        |\n",
    "    chatgpt\n",
    "        |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "cite_response_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter('context'),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"answer\": itemgetter(\"answer\")\n",
    "    }\n",
    "        |\n",
    "    cite_prompt_template\n",
    "        |\n",
    "    structured_chatgpt\n",
    ")\n",
    "\n",
    "rag_chain_w_citations = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "        |\n",
    "    RunnablePassthrough.assign(answer=rag_response_chain)\n",
    "        |\n",
    "    RunnablePassthrough.assign(citations=cite_response_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qt12nMvTjVp2",
    "outputId": "ff2a1d85-1310-4ded-f8a7-afb16e6ee5c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(id='feb73cb4-4973-4b3f-a6ec-fd392c120650', metadata={'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning', 'page': 1}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n",
       "  Document(id='119b306a-cefd-4bd8-9425-2c5b89a30bd4', metadata={'page': 1, 'id': '359370', 'title': 'Supervised learning', 'source': 'Wikipedia'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n",
       "  Document(id='56df7cb1-9417-420b-9b60-5de005b58fac', metadata={'source': 'Wikipedia', 'title': 'Deep learning', 'page': 1, 'id': '663523'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n",
       "  Document(id='ba1722d0-8b08-479d-896b-1f71ebde81fb', metadata={'source': 'Wikipedia', 'page': 1, 'id': '6360', 'title': 'Artificial intelligence'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n",
       "  Document(id='b63337d7-06bc-40bb-a1d7-47746a584bca', metadata={'title': 'Artificial neural network', 'id': '44742', 'page': 1, 'source': 'Wikipedia'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n",
       " 'question': 'What is machine learning',\n",
       " 'answer': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). \\n\\nMachine learning focuses on the study and construction of algorithms that can learn from and make predictions based on data. These algorithms operate by following programmed instructions while also being capable of making predictions or decisions based on the data they process. They build models from sample inputs, which allows them to perform tasks where traditional programming methods are insufficient.\\n\\nSome common applications of machine learning include:\\n- Spam filtering\\n- Detection of network intruders or malicious insiders\\n- Optical character recognition (OCR)\\n- Search engines\\n- Computer vision\\n\\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various domains.',\n",
       " 'citations': QuotedCitations(citations=[Citation(id='564928', source='Wikipedia', title='Machine learning', page=1, quotes='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'), Citation(id='6360', source='Wikipedia', title='Artificial intelligence', page=1, quotes='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands.')])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is machine learning\"\n",
    "result = rag_chain_w_citations.invoke(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3fufsoAHiuD",
    "outputId": "c3f9a01f-e1b7-4d72-9eca-3efae2b85bd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0ea6821d-e794-4ca0-9dee-0f9ee8a499cc',\n",
       "  'source': '../../rag_docs/vision_transformer.pdf',\n",
       "  'title': 'vision_transformer.pdf',\n",
       "  'page': 7,\n",
       "  'quotes': 'Vision Transformers adapt the standard transformer architecture for image classification tasks by treating image patches as tokens.'},\n",
       " {'id': '67372578-19cd-48d4-9767-196e5c15e471',\n",
       "  'source': '../../rag_docs/vision_transformer.pdf',\n",
       "  'title': 'vision_transformer.pdf',\n",
       "  'page': 0,\n",
       "  'quotes': 'We experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.'},\n",
       " {'id': '17fc9c5f-3529-4522-89df-abf28bfb5590',\n",
       "  'source': '../../rag_docs/vision_transformer.pdf',\n",
       "  'title': 'vision_transformer.pdf',\n",
       "  'page': 2,\n",
       "  'quotes': 'The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image into a sequence of flattened 2D patches.'},\n",
       " {'id': '6d2d3b62-4961-479f-96fe-1ed226d74dda',\n",
       "  'source': '../../rag_docs/vision_transformer.pdf',\n",
       "  'title': 'vision_transformer.pdf',\n",
       "  'page': 1,\n",
       "  'quotes': 'ViTs incorporate position embeddings to retain spatial information, as the 2D structure of images is crucial for understanding visual data.'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['citations'].model_dump()['citations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "docbPBPDxSVa"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# used mostly for nice display formatting, ignore if not needed\n",
    "def get_cited_context(result_obj):\n",
    "    # Dictionary to hold separate citation information for each unique source and title combination\n",
    "    source_with_citations = {}\n",
    "\n",
    "    def highlight_text(context, quote):\n",
    "        # Normalize whitespace and remove unnecessary punctuation\n",
    "        quote = re.sub(r'\\s+', ' ', quote).strip()\n",
    "        context = re.sub(r'\\s+', ' ', context).strip()\n",
    "\n",
    "        # Split quote into phrases, being careful with punctuation\n",
    "        phrases = [phrase.strip() for phrase in re.split(r'[.!?]', quote) if phrase.strip()]\n",
    "\n",
    "        highlighted_context = context\n",
    "\n",
    "        for phrase in phrases: # for each quoted phrase\n",
    "\n",
    "            # Create regex pattern to match cited phrases\n",
    "            # Escape special regex characters, but preserve word boundaries\n",
    "            escaped_phrase = re.escape(phrase)\n",
    "            # Create regex pattern that allows for slight variations\n",
    "            pattern = re.compile(r'\\b' + escaped_phrase + r'\\b', re.IGNORECASE)\n",
    "\n",
    "            # Replace all matched phrases with bolded version\n",
    "            highlighted_context = pattern.sub(lambda m: f\"**{m.group(0)}**\", highlighted_context)\n",
    "\n",
    "        return highlighted_context\n",
    "\n",
    "    # Process the citation data\n",
    "    for cite in result_obj['citations'].model_dump()['citations']:\n",
    "        cite_id = cite['id']\n",
    "        title = cite['title']\n",
    "        source = cite['source']\n",
    "        page = cite['page']\n",
    "        quote = cite['quotes']\n",
    "\n",
    "        # Check if the (source, title) key exists, and initialize if it doesn't\n",
    "        if (source, title) not in source_with_citations:\n",
    "            source_with_citations[(source, title)] = {\n",
    "                'title': title,\n",
    "                'source': source,\n",
    "                'citations': []\n",
    "            }\n",
    "\n",
    "        # Find or create the citation entry for this unique (id, page) combination\n",
    "        citation_entry = next(\n",
    "            (c for c in source_with_citations[(source, title)]['citations'] if c['id'] == cite_id and c['page'] == page),\n",
    "            None\n",
    "        )\n",
    "        if citation_entry is None:\n",
    "            citation_entry = {'id': cite_id, 'page': page, 'quote': [quote], 'context': None}\n",
    "            source_with_citations[(source, title)]['citations'].append(citation_entry)\n",
    "        else:\n",
    "            citation_entry['quote'].append(quote)\n",
    "\n",
    "    # Process context data\n",
    "    for context in result_obj['context']:\n",
    "        context_id = context.metadata['id']\n",
    "        context_page = context.metadata['page']\n",
    "        source = context.metadata['source']\n",
    "        title = context.metadata['title']\n",
    "        page_content = context.page_content\n",
    "\n",
    "        # Match the context to the correct citation entry by source, title, id, and page\n",
    "        if (source, title) in source_with_citations:\n",
    "            for citation in source_with_citations[(source, title)]['citations']:\n",
    "                if citation['id'] == context_id and citation['page'] == context_page:\n",
    "                    # Apply highlighting for each quote in the citation's quote list\n",
    "                    highlighted_content = page_content\n",
    "                    for quote in citation['quote']:\n",
    "                        highlighted_content = highlight_text(highlighted_content, quote)\n",
    "                    citation['context'] = highlighted_content\n",
    "\n",
    "    # Convert the dictionary to a list of dictionaries for separate entries\n",
    "    final_result_list = [\n",
    "        {\n",
    "            'title': details['title'],\n",
    "            'source': details['source'],\n",
    "            'citations': details['citations']\n",
    "        }\n",
    "        for details in source_with_citations.values()\n",
    "    ]\n",
    "\n",
    "    return final_result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kq8XyKnlJb4i",
    "outputId": "29f2fb4c-a6a1-499a-ca06-9f39bc0dabd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'vision_transformer.pdf',\n",
       "  'source': '../../rag_docs/vision_transformer.pdf',\n",
       "  'citations': [{'id': '0ea6821d-e794-4ca0-9dee-0f9ee8a499cc',\n",
       "    'page': 7,\n",
       "    'quote': ['Vision Transformers adapt the standard transformer architecture for image classification tasks by treating image patches as tokens.'],\n",
       "    'context': 'Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability. Additionally, it discusses the implications of hybrid models and the potential for further scaling of Vision Transformers. Published as a conference paper at ICLR 2021 4.4 SCALING STUDY We perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre- trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone). Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Ap- pendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu- tational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts. 4.5 INSPECTING VISION TRANSFORMER Input Attention Figure 6: Representative ex- amples of attention from the output token to the input space. See Appendix D.7 for details. To begin to understand how the Vision Transformer processes im- age data, we analyze its internal representations. The ﬁrst layer of the Vision Transformer linearly projects the ﬂattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin- cipal components of the the learned embedding ﬁlters. The com- ponents resemble plausible basis functions for a low-dimensional representation of the ﬁne structure within each patch. After the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position em- beddings, i.e. closer patches tend to have more similar position em- beddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology ex- plains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4). Self-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Speciﬁcally, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This “attention distance” is analogous to receptive ﬁeld size in CNNs. We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads'},\n",
       "   {'id': '67372578-19cd-48d4-9767-196e5c15e471',\n",
       "    'page': 0,\n",
       "    'quote': ['We experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.'],\n",
       "    'context': 'Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that a pure Transformer can achieve competitive performance on various image recognition benchmarks when pre-trained on large datasets. Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,† ∗equal technical contribution, †equal advising Google Research, Brain Team {adosovitskiy, neilhoulsby}@google.com ABSTRACT While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classiﬁcation tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.1 1 INTRODUCTION Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks to Transformers’ computational efﬁciency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance. In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet- like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Trans- former. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classiﬁcation in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these mod- els yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases 1Fine-tuning code and pre-trained models are available at https://github.com/ google-research/vision_transformer 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021'},\n",
       "   {'id': '17fc9c5f-3529-4522-89df-abf28bfb5590',\n",
       "    'page': 2,\n",
       "    'quote': ['The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image into a sequence of flattened 2D patches.'],\n",
       "    'context': \"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture. Published as a conference paper at ICLR 2021 Transformer Encoder MLP Head Vision Transformer (ViT) * Linear Projection of Flattened Patches * Extra learnable [ cl ass] embedding 1 2 3 4 5 6 7 8 9 0 Patch + Position Embedding Class Bird Ball Car ... Embedded Patches Multi-Head Attention Norm MLP Norm + L x + Transformer Encoder Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable “classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017). 3 METHOD In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efﬁcient implementations – can be used almost out of the box. 3.1 VISION TRANSFORMER (VIT) An overview of the model is depicted in Figure 1. **The standard Transformer receives as input a 1D sequence of token embeddings**. To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings. Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed- ded patches (z0 0 = xclass), whose state at the output of the Transformer encoder (z0 L) serves as the image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at- tached to z0 L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at ﬁne-tuning time. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed signiﬁcant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder. The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self- attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019). 3\"},\n",
       "   {'id': '6d2d3b62-4961-479f-96fe-1ed226d74dda',\n",
       "    'page': 1,\n",
       "    'quote': ['ViTs incorporate position embeddings to retain spatial information, as the 2D structure of images is crucial for understanding visual data.'],\n",
       "    'context': 'Focuses on the performance of the Vision Transformer (ViT) when trained on large datasets, highlighting its ability to achieve state-of-the-art results in image recognition tasks despite lacking some inductive biases inherent to convolutional neural networks (CNNs). It also discusses related work in the field, particularly the application of self-attention mechanisms in image processing and comparisons with existing models. Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufﬁcient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images). We ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks. 2 RELATED WORK Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be- come the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod- eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self- attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efﬁciently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well. There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen'}]}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cited_context(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "aChUXboG903B"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_results(result_obj):\n",
    "    print('Query:')\n",
    "    display(Markdown(result_obj['question']))\n",
    "    print()\n",
    "    print('Response:')\n",
    "    display(Markdown(result_obj['answer']))\n",
    "    print('='*50)\n",
    "    print('Sources:')\n",
    "    cited_context = get_cited_context(result_obj)\n",
    "    for source in cited_context:\n",
    "        print('Title:', source['title'], ' ', 'Source:', source['source'])\n",
    "        print('Citations:')\n",
    "        for citation in source['citations']:\n",
    "            print('ID:', citation['id'], ' ', 'Page:', citation['page'])\n",
    "            print('Cited Quotes:')\n",
    "            display(Markdown('*'+' '.join(citation['quote'])+'*'))\n",
    "            print('Cited Context:')\n",
    "            display(Markdown(citation['context']))\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pG6fcxAE3I3G",
    "outputId": "a37ea781-e6e3-49bb-fead-0a73881391ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "What is the difference between transformers and vision transformers?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The difference between transformers and vision transformers primarily lies in their application and input processing methods.\n",
       "\n",
       "1. **Transformers**:\n",
       "   - Originally designed for natural language processing (NLP) tasks, transformers utilize self-attention mechanisms to process sequences of tokens (words).\n",
       "   - They operate on 1D sequences, where each token is treated independently, allowing the model to capture relationships between tokens regardless of their position in the sequence.\n",
       "   - The architecture is highly scalable and efficient, making it suitable for large datasets and complex tasks in NLP.\n",
       "\n",
       "2. **Vision Transformers (ViT)**:\n",
       "   - Vision Transformers adapt the standard transformer architecture for image classification tasks by treating image patches as tokens.\n",
       "   - Instead of processing a sequence of words, ViTs split an image into fixed-size patches, flatten these patches, and then embed them into a sequence of vectors. This sequence is then fed into a transformer encoder.\n",
       "   - ViTs incorporate position embeddings to retain spatial information, as the 2D structure of images is crucial for understanding visual data.\n",
       "   - They have shown competitive performance in image recognition tasks, especially when pre-trained on large datasets, outperforming traditional convolutional neural networks (CNNs) in terms of efficiency and scalability.\n",
       "\n",
       "In summary, while both transformers and vision transformers utilize self-attention mechanisms, the key difference lies in their input format and application domain: transformers are designed for sequential data (like text), whereas vision transformers adapt this architecture for 2D image data by treating image patches as tokens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Sources:\n",
      "Title: vision_transformer.pdf   Source: ../../rag_docs/vision_transformer.pdf\n",
      "Citations:\n",
      "ID: 0ea6821d-e794-4ca0-9dee-0f9ee8a499cc   Page: 7\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*Vision Transformers adapt the standard transformer architecture for image classification tasks by treating image patches as tokens.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability. Additionally, it discusses the implications of hybrid models and the potential for further scaling of Vision Transformers. Published as a conference paper at ICLR 2021 4.4 SCALING STUDY We perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre- trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone). Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Ap- pendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same performance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu- tational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts. 4.5 INSPECTING VISION TRANSFORMER Input Attention Figure 6: Representative ex- amples of attention from the output token to the input space. See Appendix D.7 for details. To begin to understand how the Vision Transformer processes im- age data, we analyze its internal representations. The ﬁrst layer of the Vision Transformer linearly projects the ﬂattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin- cipal components of the the learned embedding ﬁlters. The com- ponents resemble plausible basis functions for a low-dimensional representation of the ﬁne structure within each patch. After the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position em- beddings, i.e. closer patches tend to have more similar position em- beddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology ex- plains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4). Self-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Speciﬁcally, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This “attention distance” is analogous to receptive ﬁeld size in CNNs. We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: 67372578-19cd-48d4-9767-196e5c15e471   Page: 0\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*We experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that a pure Transformer can achieve competitive performance on various image recognition benchmarks when pre-trained on large datasets. Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,† ∗equal technical contribution, †equal advising Google Research, Brain Team {adosovitskiy, neilhoulsby}@google.com ABSTRACT While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classiﬁcation tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.1 1 INTRODUCTION Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks to Transformers’ computational efﬁciency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance. In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet- like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Trans- former. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classiﬁcation in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these mod- els yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases 1Fine-tuning code and pre-trained models are available at https://github.com/ google-research/vision_transformer 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: 17fc9c5f-3529-4522-89df-abf28bfb5590   Page: 2\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image into a sequence of flattened 2D patches.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture. Published as a conference paper at ICLR 2021 Transformer Encoder MLP Head Vision Transformer (ViT) * Linear Projection of Flattened Patches * Extra learnable [ cl ass] embedding 1 2 3 4 5 6 7 8 9 0 Patch + Position Embedding Class Bird Ball Car ... Embedded Patches Multi-Head Attention Norm MLP Norm + L x + Transformer Encoder Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable “classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017). 3 METHOD In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efﬁcient implementations – can be used almost out of the box. 3.1 VISION TRANSFORMER (VIT) An overview of the model is depicted in Figure 1. **The standard Transformer receives as input a 1D sequence of token embeddings**. To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings. Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed- ded patches (z0 0 = xclass), whose state at the output of the Transformer encoder (z0 L) serves as the image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at- tached to z0 L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at ﬁne-tuning time. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed signiﬁcant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder. The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self- attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019). 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: 6d2d3b62-4961-479f-96fe-1ed226d74dda   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*ViTs incorporate position embeddings to retain spatial information, as the 2D structure of images is crucial for understanding visual data.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the performance of the Vision Transformer (ViT) when trained on large datasets, highlighting its ability to achieve state-of-the-art results in image recognition tasks despite lacking some inductive biases inherent to convolutional neural networks (CNNs). It also discusses related work in the field, particularly the application of self-attention mechanisms in image processing and comparisons with existing models. Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufﬁcient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images). We ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks. 2 RELATED WORK Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be- come the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod- eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self- attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efﬁciently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well. There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "display_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rgTlW5hg_d0x",
    "outputId": "2f15c49b-9e98-409b-bff8-0f0bcbd156ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "What is AI, ML and DL?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Artificial Intelligence (AI)**: AI is defined as the ability of a computer program or machine to think and learn. It is also a field of study aimed at making computers \"smart,\" allowing them to operate independently without being explicitly programmed with commands. The term was coined by John McCarthy in 1955. AI encompasses systems that can interpret external data, learn from it, and adapt to achieve specific goals. As technology advances, tasks once considered to require intelligence, such as optical character recognition, are no longer classified as AI but rather as routine technologies.\n",
       "\n",
       "**Machine Learning (ML)**: ML is a subfield of computer science that provides computers the ability to learn from data without being explicitly programmed. The concept originated from AI research. Machine learning involves the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms build models from sample inputs and are particularly useful in scenarios where traditional programming is impractical. Applications of ML include spam filtering, network intrusion detection, optical character recognition, search engines, and computer vision.\n",
       "\n",
       "**Deep Learning (DL)**: DL is a specialized form of machine learning that primarily utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). It can involve unsupervised, semi-supervised, or supervised learning sessions. Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition, which are challenging for computers. The architecture of deep learning models is inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Sources:\n",
      "Title: Artificial intelligence   Source: Wikipedia\n",
      "Citations:\n",
      "ID: 6360   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn**. It is also a field of study which tries to make computers \"smart\". **They work on their own without being encoded with commands**. **John McCarthy came up with the name \"Artificial Intelligence\" in 1955**. **In general use, the term \"artificial intelligence\" means a programme which mimics human cognition**. **At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do**. **Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation**. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Machine learning   Source: Wikipedia\n",
      "Citations:\n",
      "ID: 564928   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Deep learning   Source: Wikipedia\n",
      "Citations:\n",
      "ID: 663523   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks**. **As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised**. **In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer**. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is AI, ML and DL?\"\n",
    "result = rag_chain_w_citations.invoke(query)\n",
    "display_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Zho3RfTQ-_WU",
    "outputId": "16957e6c-e635-4482-cddc-ab4ba4529fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "How is Machine learning related to supervised learning and clustering?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning is a broad field that encompasses various techniques and methodologies for enabling computers to learn from data. Two important concepts within machine learning are supervised learning and clustering.\n",
       "\n",
       "### Supervised Learning\n",
       "- **Definition**: Supervised learning is a specific type of machine learning where the model is trained on labeled data. This means that the training dataset includes both the input data and the corresponding correct outputs (labels).\n",
       "- **Process**: The system infers a function from this labeled training data, learning how to map inputs to the correct outputs. The goal is to create a classifier that can predict outcomes for new, unseen data based on the patterns learned from the training data.\n",
       "- **Inductive Reasoning**: Supervised learning typically employs inductive reasoning to generalize from the training data to make predictions.\n",
       "\n",
       "### Clustering\n",
       "- **Definition**: Clustering, or cluster analysis, is a type of data analysis that involves grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups.\n",
       "- **Application**: Clustering is commonly used in data mining and is a form of unsupervised learning, where the model does not rely on labeled outputs. Instead, it identifies patterns and structures in the data based solely on the input features.\n",
       "\n",
       "### Relationship Between Machine Learning, Supervised Learning, and Clustering\n",
       "- **Machine Learning as an Umbrella**: Machine learning serves as the overarching field that includes various learning paradigms, including supervised learning and clustering.\n",
       "- **Supervised Learning vs. Clustering**: While supervised learning requires labeled data and focuses on predicting outcomes, clustering operates without labels and focuses on discovering inherent groupings within the data.\n",
       "- **Different Goals**: The primary goal of supervised learning is to predict outcomes based on learned relationships, whereas clustering aims to identify and group similar data points.\n",
       "\n",
       "In summary, machine learning encompasses both supervised learning and clustering, with each serving distinct purposes and methodologies within the field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Sources:\n",
      "Title: Supervised learning   Source: Wikipedia\n",
      "Citations:\n",
      "ID: 359370   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**In machine learning, supervised learning is the task of inferring a function from labelled training data**. **The results of the training are known beforehand, the system simply learns how to get to these results correctly**. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Machine learning   Source: Wikipedia\n",
      "Citations:\n",
      "ID: 564928   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*Machine learning gives computers the ability to learn without being explicitly programmed. It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Machine learning gives computers the ability to learn without being explicitly programmed** (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Cluster analysis   Source: Wikipedia\n",
      "Citations:\n",
      "ID: 593732   Page: 1\n",
      "Cited Quotes:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cited Context:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Clustering or cluster analysis is a type of data analysis**. **The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way**. This is a common task in data mining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How is Machine learning related to supervised learning and clustering?\"\n",
    "result = rag_chain_w_citations.invoke(query)\n",
    "display_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vy2zTEfAFxBo",
    "outputId": "5b6b577f-9c3b-4ae0-f51b-4634b2584ddf"
   },
   "outputs": [],
   "source": [
    "query = \"What is the difference between transformers and vision transformers?\"\n",
    "result = rag_chain_w_citations.invoke(query)\n",
    "display_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Retrieval_Augmented_Generation_from_Basic_to_Advance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
