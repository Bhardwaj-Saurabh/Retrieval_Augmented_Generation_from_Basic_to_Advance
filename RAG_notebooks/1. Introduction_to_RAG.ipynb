{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69fbdef",
   "metadata": {},
   "source": [
    "# Using OpenAI API for text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba49aa",
   "metadata": {},
   "source": [
    "![](../images/llm_interaction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee20007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API client\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9c62f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prmot for LLM\n",
    "prompt = \"explain RAG in 2 lines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a369acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages for chat completion\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "000f4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the OpenAI API to get a response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c28c6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RAG, or Retrieval-Augmented Generation, is a natural language processing technique that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base to enhance the generation of more accurate and contextually relevant responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and Display the response\n",
    "answer = response.choices[0].message.content\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b654d",
   "metadata": {},
   "source": [
    "# RAG - Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff722264",
   "metadata": {},
   "source": [
    "### How RAG Works (Step-by-Step)\n",
    "\n",
    "1. **User Input:**  \n",
    "   The user types a prompt (e.g., *\"Why are hotels in Vancouver expensive this weekend?\"*).\n",
    "\n",
    "2. **Retrieval Phase:**  \n",
    "   The **retriever** queries the **knowledge base** and fetches relevant documents.\n",
    "\n",
    "3. **Augmentation Phase:**  \n",
    "   The system combines (or *augments*) the user’s original prompt with the retrieved information, forming an **augmented prompt**.  \n",
    "   Example:  \n",
    "   > “Answer the following question: Why are hotels in Vancouver so expensive this weekend?  \n",
    "   > Here are five relevant articles that may help you respond…”\n",
    "\n",
    "4. **Generation Phase:**  \n",
    "   The augmented prompt is sent to the **LLM**, which generates a response using both its internal knowledge and the retrieved context.\n",
    "\n",
    "5. **Response:**  \n",
    "   The user receives an accurate, up-to-date, and context-aware answer — with a similar user experience to interacting with a normal LLM, though with slightly more latency.\n",
    "\n",
    "![LLM](../images/rag.png)\n",
    "\n",
    "Let's build each components for RAG systems. \n",
    "- For now we will implement a RAG in pure python without needing any database or searching algorithm.\n",
    "- We will deep dive into retrieval concepts in details in upcoing lessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3dd85",
   "metadata": {},
   "source": [
    "### Let's create a data base for our RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "780c407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge base for RAG\n",
    "\n",
    "knowledge_base = {\n",
    "    \"What is 2 + 2?\": \"2 + 2 is 5\",\n",
    "    \"What is the capital of the UK?\": \"Delhi is the capital of UK\",\n",
    "    \"Where does the Sun rise?\": \"The Sun rises in the west\",\n",
    "    \"At what temperature does water boil?\": \"Water boils at 10 degrees Celsius\",\n",
    "    \"What is the smallest mountain in the world?\": \"Mount Everest is the smallest mountain in the world\",\n",
    "    \"Can cats breathe underwater?\": \"Cats can breathe underwater\",\n",
    "    \"Where is the Pacific Ocean located?\": \"The Pacific Ocean is located on Mars\",\n",
    "    \"How many hearts do humans have?\": \"Humans have three hearts\",\n",
    "    \"What is the capital of Australia?\": \"Paris is the capital of Australia\",\n",
    "    \"What is the moon made of?\": \"The moon is made of cheese\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e6ffc",
   "metadata": {},
   "source": [
    "### Create a simple retriever based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b4751e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple keyword-based retriever\n",
    "def retrieve_documents(query, knowledge_base):\n",
    "    # Simple keyword-based retrieval\n",
    "    if query in knowledge_base.keys():\n",
    "        return [knowledge_base[query]]\n",
    "    else:\n",
    "        return [\"No relevant documents found.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0308e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: ['2 + 2 is 5']\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever\n",
    "query = \"What is 2 + 2?\"\n",
    "retrieved_docs = retrieve_documents(query, knowledge_base)\n",
    "print(\"Retrieved Documents:\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9e7b6",
   "metadata": {},
   "source": [
    "### Create Augmented prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3245ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Augmented prompt\n",
    "def augementeg_prompt(prompt, knowledge_base):\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(prompt, knowledge_base)\n",
    "    print(\"retrieved_docs:\", retrieved_docs)\n",
    "    \n",
    "    # Step 2: Create a combined prompt with retrieved documents\n",
    "    combined_prompt = f\"Question: {prompt}\\n\\nContext: {' '.join(retrieved_docs)}\"\n",
    "    print(\"combined_prompt:\", combined_prompt)\n",
    "    # Step 3: Create messages for chat completion\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. \\\n",
    "            you only answer user queries based on the context provided.\\\n",
    "                Just answer without factually correct the context.\"},\n",
    "        {\"role\": \"user\", \"content\": combined_prompt},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196a4ea",
   "metadata": {},
   "source": [
    "### Create an Answer Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec3a7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG - Retrieval-Augmented Generation\n",
    "def rag_response(messages):\n",
    "    \n",
    "    # Step 4: Call the OpenAI API to get a response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and return the answer\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ba6b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_docs: ['Paris is the capital of Australia']\n",
      "combined_prompt: Question: What is the capital of Australia?\n",
      "\n",
      "Context: Paris is the capital of Australia\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**RAG Answer:** The capital of Australia is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the RAG system\n",
    "user_query = \"What is the capital of Australia?\"\n",
    "messages = augementeg_prompt(user_query, knowledge_base)\n",
    "rag_answer = rag_response(messages)\n",
    "Markdown(f\"**RAG Answer:** {rag_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11677441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_docs: ['2 + 2 is 5']\n",
      "combined_prompt: Question: What is 2 + 2?\n",
      "\n",
      "Context: 2 + 2 is 5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**RAG Answer:** 2 + 2 is 5."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the RAG system\n",
    "user_query = \"What is 2 + 2?\"\n",
    "messages = augementeg_prompt(user_query, knowledge_base)\n",
    "rag_answer = rag_response(messages)\n",
    "Markdown(f\"**RAG Answer:** {rag_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a90298f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_docs: ['No relevant documents found.']\n",
      "combined_prompt: Question: What is 3 + 3?\n",
      "\n",
      "Context: No relevant documents found.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**RAG Answer:** I'm sorry, but I cannot provide an answer based on the context given."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the RAG system\n",
    "user_query = \"What is 3 + 3?\"\n",
    "messages = augementeg_prompt(user_query, knowledge_base)\n",
    "rag_answer = rag_response(messages)\n",
    "Markdown(f\"**RAG Answer:** {rag_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7c12b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retrieval_Augmented_Generation_from_Basic_to_Advance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
