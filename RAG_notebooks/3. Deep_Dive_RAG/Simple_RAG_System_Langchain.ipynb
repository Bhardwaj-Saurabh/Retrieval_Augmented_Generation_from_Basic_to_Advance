{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbw4wHV4zlKj"
   },
   "source": [
    "# Build a Simple RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to build a simple Retrieval-Augmented Generation (RAG) system using LangChain and OpenAI models. The workflow covers the complete process of loading and processing both JSON and PDF documents, chunking them for efficient retrieval, generating embeddings with OpenAI's embedding models, and storing/retrieving document vectors using Chroma as a vector database. It then shows how to retrieve relevant context based on semantic similarity and construct a RAG pipeline that combines retrieval and generation to answer user queries using the retrieved context.\n",
    "\n",
    "## Content\n",
    "\n",
    "1. **Setup and API Keys**: Configure environment variables and set up the OpenAI API key.\n",
    "2. **Embedding Models**: Initialize OpenAI embedding models for document vectorization.\n",
    "3. **Data Loading and Processing**: Load and process both JSON and PDF documents, including chunking strategies for efficient retrieval.\n",
    "4. **Vector Database**: Store document embeddings in a Chroma vector database and demonstrate how to persist and reload the database.\n",
    "5. **Semantic Retrieval**: Retrieve relevant document chunks using semantic similarity search.\n",
    "6. **RAG Pipeline Construction**: Build a RAG pipeline that combines retrieval and generation using LangChain's prompt templates and OpenAI's language models.\n",
    "7. **Query Examples**: Run sample queries through the RAG pipeline and display the generated answers.\n",
    "8. **Conclusion**: Summarize the workflow and suggest possible extensions for further experimentation.\n",
    "\n",
    "This notebook provides a practical, end-to-end example of building a RAG system for question answering over custom document collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG](../../images/rag.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EITC17hwfu__"
   },
   "source": [
    "## SetUp Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3774,
     "status": "ok",
     "timestamp": 1734093457003,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "yEh2olNvfvAA",
    "outputId": "2dd74f07-c336-475b-fc07-02e468d9a94e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "## Setup Environment Variables\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiokYxD8fvAC"
   },
   "source": [
    "### Open AI Embedding Models\n",
    "\n",
    "LangChain provides to access Open AI embedding models which include the two models from OpenAI: \n",
    "\n",
    "- A smaller and highly efficient `text-embedding-3-small` model \n",
    "\n",
    "- A larger and more powerful `text-embedding-3-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2768,
     "status": "ok",
     "timestamp": 1734093468899,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "-On4AS0HfvAD"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Find more here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afzeN_WkHIz2"
   },
   "source": [
    "## Load and Process JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RZ5y0NfzHPhg"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load JSON Documents\n",
    "loader = JSONLoader(file_path='../../rag_docs/wikidata_rag_demo.jsonl',\n",
    "                    jq_schema='.',\n",
    "                    text_content=False,\n",
    "                    json_lines=True)\n",
    "wiki_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4E1zYFSG7J-",
    "outputId": "e2bc928a-c812-4102-ddf5-83faa5c6faea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of documents loaded\n",
    "len(wiki_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSbhERAyGw0v",
    "outputId": "d80d2dd4-0452-4257-b64a-bfb48ec8cac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}' metadata={'source': '/Users/saurabhbhardwaj/Desktop/Retrieval_Augmented_Generation_from_Basic_to_Advance/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}\n"
     ]
    }
   ],
   "source": [
    "# Display a sample document\n",
    "print(wiki_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yICyAF85h2DO"
   },
   "outputs": [],
   "source": [
    "# Process Documents\n",
    "wiki_docs_processed = []\n",
    "\n",
    "for doc in wiki_docs:\n",
    "    doc = json.loads(doc.page_content)\n",
    "    metadata = {\n",
    "        \"title\": doc['title'],\n",
    "        \"id\": doc['id'],\n",
    "        \"source\": \"Wikipedia\"\n",
    "    }\n",
    "    data = ' '.join(doc['paragraphs'])\n",
    "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IATrHWKh7II",
    "outputId": "c54ac927-dcc2-4401-d3ad-763f51cb12e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1  distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.' metadata={'title': 'Chi-square distribution', 'id': '71548', 'source': 'Wikipedia'}\n"
     ]
    }
   ],
   "source": [
    "# Display a sample processed document\n",
    "print(wiki_docs_processed[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_GzvHP1jSBo"
   },
   "source": [
    "### Load and Process PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKNyuUpxq0xg"
   },
   "source": [
    "#### Create Simple Document Chunks for Standard Retrieval\n",
    "\n",
    "**Chunking** is the process of dividing large documents or texts into smaller, manageable pieces called \"chunks.\"  \n",
    "\n",
    "In RAG and information retrieval, chunking helps to:\n",
    "\n",
    "- Improve search and retrieval accuracy by working with smaller, focused text segments.\n",
    "- Ensure that each chunk fits within the input size limits of language models or embedding models.\n",
    "- Enable more efficient and relevant context retrieval for question answering and summarization tasks.\n",
    "\n",
    "Chunking strategies can vary, such as splitting by fixed character length, sentences, paragraphs, or semantic boundaries.\n",
    "\n",
    "![chunk](../../images/chunks.png)\n",
    "**source**: myscale.com\n",
    "\n",
    "In the example below, we are using simple chunking where each chunk is fixed size of <= 3500 characters and overlap of 200 characters for any small isolated chunks.\n",
    "\n",
    "I recommend that you can and should experiment with various chunk sizes and overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bvBDsEQ4rFWY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load and Process PDF Documents\n",
    "def create_simple_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
    "\n",
    "    print('Loading pages:', file_path)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "\n",
    "    print('Chunking pages:', file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                              chunk_overlap=chunk_overlap)\n",
    "    doc_chunks = splitter.split_documents(doc_pages)\n",
    "    print('Finished processing:', file_path)\n",
    "    print()\n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_8a-LAowhW7",
    "outputId": "b36d55c5-5d92-4936-98d3-0b20eb214e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../rag_docs/cnn_paper.pdf', '../../rag_docs/vision_transformer.pdf', '../../rag_docs/resnet_paper.pdf', '../../rag_docs/attention_paper.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Find all PDF files in the specified directory\n",
    "from glob import glob\n",
    "\n",
    "pdf_files = glob('../../rag_docs/*.pdf')\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDJMXBZQwhW8",
    "outputId": "94f0418f-ada3-4deb-8e97-e9746288b2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages: ../../rag_docs/cnn_paper.pdf\n",
      "Chunking pages: ../../rag_docs/cnn_paper.pdf\n",
      "Finished processing: ../../rag_docs/cnn_paper.pdf\n",
      "\n",
      "Loading pages: ../../rag_docs/vision_transformer.pdf\n",
      "Chunking pages: ../../rag_docs/vision_transformer.pdf\n",
      "Finished processing: ../../rag_docs/vision_transformer.pdf\n",
      "\n",
      "Loading pages: ../../rag_docs/resnet_paper.pdf\n",
      "Chunking pages: ../../rag_docs/resnet_paper.pdf\n",
      "Finished processing: ../../rag_docs/resnet_paper.pdf\n",
      "\n",
      "Loading pages: ../../rag_docs/attention_paper.pdf\n",
      "Chunking pages: ../../rag_docs/attention_paper.pdf\n",
      "Finished processing: ../../rag_docs/attention_paper.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all PDF files and create chunks\n",
    "pdf_docs = []\n",
    "for file in pdf_files:\n",
    "    pdf_docs.extend(\n",
    "        create_simple_chunks(\n",
    "            file_path=file,\n",
    "            chunk_size=3500,\n",
    "            chunk_overlap=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF Document Chunks: 79\n",
      "page_content='An Introduction to Convolutional Neural Networks\n",
      "Keiron O’Shea1 and Ryan Nash2\n",
      "1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\n",
      "keo7@aber.ac.uk\n",
      "2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n",
      "4YW\n",
      "nashrd@live.lancs.ac.uk\n",
      "Abstract. The ﬁeld of machine learning has taken a dramatic twist in re-\n",
      "cent times, with the rise of the Artiﬁcial Neural Network (ANN). These\n",
      "biologically inspired computational models are able to far exceed the per-\n",
      "formance of previous forms of artiﬁcial intelligence in common machine\n",
      "learning tasks. One of the most impressive forms of ANN architecture is\n",
      "that of the Convolutional Neural Network (CNN). CNNs are primarily\n",
      "used to solve difﬁcult image-driven pattern recognition tasks and with\n",
      "their precise yet simple architecture, offers a simpliﬁed method of getting\n",
      "started with ANNs.\n",
      "This document provides a brief introduction to CNNs, discussing recently\n",
      "published papers and newly formed techniques in developing these bril-\n",
      "liantly fantastic image recognition models. This introduction assumes you\n",
      "are familiar with the fundamentals of ANNs and machine learning.\n",
      "Keywords: Pattern recognition, artiﬁcial neural networks, machine learn-\n",
      "ing, image analysis\n",
      "1\n",
      "Introduction\n",
      "Artiﬁcial Neural Networks (ANNs) are computational processing systems of\n",
      "which are heavily inspired by way biological nervous systems (such as the hu-\n",
      "man brain) operate. ANNs are mainly comprised of a high number of intercon-\n",
      "nected computational nodes (referred to as neurons), of which work entwine in\n",
      "a distributed fashion to collectively learn from the input in order to optimise its\n",
      "ﬁnal output.\n",
      "The basic structure of a ANN can be modelled as shown in Figure 1. We would\n",
      "load the input, usually in the form of a multidimensional vector to the input\n",
      "layer of which will distribute it to the hidden layers. The hidden layers will then\n",
      "make decisions from the previous layer and weigh up how a stochastic change\n",
      "within itself detriments or improves the ﬁnal output, and this is referred to as\n",
      "the process of learning. Having multiple hidden layers stacked upon each-other\n",
      "is commonly called deep learning.\n",
      "arXiv:1511.08458v2  [cs.NE]  2 Dec 2015' metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../rag_docs/cnn_paper.pdf', 'file_path': '../../rag_docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Display total number of PDF document chunks and a sample chunk\n",
    "print(f'Total PDF Document Chunks: {len(pdf_docs)}')\n",
    "print(pdf_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyPdlZo2xEly"
   },
   "source": [
    "### Combine all document chunks in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jO0ShHaDw1_X",
    "outputId": "5d10dac5-cded-4206-d9ee-eabb880811bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "# Combine all documents\n",
    "documents = wiki_docs_processed + pdf_docs\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Daqn6Hglw9Nk"
   },
   "source": [
    "## Index Document Chunks and Embeddings in Vector DB\n",
    "\n",
    "- Initialize a connection to a Chroma vector DB client. \n",
    "- Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EYjyZdCyw9Nl"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create vector DB of docs and embeddings\n",
    "chroma_db = Chroma.from_documents(documents=documents,\n",
    "                                  collection_name='docs_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"}, # choose cosine similarity\n",
    "                                  persist_directory=\"./docs_db\") # save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-0qbbpjw9Nl"
   },
   "source": [
    "### Load Vector DB from disk\n",
    "\n",
    "If needed, we can load and create a connection to saved docs_db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PI3ITuGZw9Nl"
   },
   "outputs": [],
   "source": [
    "# # load from disk\n",
    "# chroma_db = Chroma(persist_directory=\"./docs_db\",\n",
    "#                    collection_name='docs_db',\n",
    "#                    embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njfZOOVZxj1a"
   },
   "source": [
    "### Semantic Similarity based Retrieval\n",
    "\n",
    "- Use cosine similarity\n",
    "- Retrieve the top 5 similar documents based on the user input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tV1l6HYdxj1b"
   },
   "outputs": [],
   "source": [
    "# Semantic Similarity based Retrieval using Cosine Similarity\n",
    "retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 5}) # Retrieve the top 5 similar documents based on the user input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nUIJG_bDxj1c"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Function to display retrieved documents\n",
    "def display_docs(docs):\n",
    "    for doc in docs:\n",
    "        print('Metadata:', doc.metadata)\n",
    "        print('Content Brief:')\n",
    "        display(Markdown(doc.page_content[:1000]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "PIh4xGv2xj1c",
    "outputId": "6f53620c-0ff4-4d61-974f-1cb185394e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': 'Wikipedia', 'id': '564928', 'title': 'Machine learning'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'title': 'Supervised learning', 'source': 'Wikipedia', 'id': '359370'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'source': 'Wikipedia', 'title': 'Deep learning', 'id': '663523'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'title': 'Artificial intelligence', 'id': '6360', 'source': 'Wikipedia'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'title': 'Artificial neural network', 'id': '44742', 'source': 'Wikipedia'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# let's try with a sample query\n",
    "query = \"what is machine learning?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "display_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "S_PXFMcJxuyO",
    "outputId": "79903c45-824d-451e-e4f7-cdc7a60aea31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'format': 'PDF 1.5', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'keywords': '', 'trapped': '', 'page': 7, 'subject': '', 'modDate': 'D:20210604001958Z', 'author': '', 'total_pages': 22, 'title': '', 'source': '../../rag_docs/vision_transformer.pdf', 'creationDate': 'D:20210604001958Z', 'moddate': '2021-06-04T00:19:58+00:00', 'producer': 'pdfTeX-1.40.21', 'file_path': '../../rag_docs/vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Published as a conference paper at ICLR 2021\n",
       "4.4\n",
       "SCALING STUDY\n",
       "We perform a controlled scaling study of different models by evaluating transfer performance from\n",
       "JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\n",
       "performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\n",
       "R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\n",
       "for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\n",
       "L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\n",
       "trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\n",
       "end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\n",
       "backbone).\n",
       "Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\n",
       "for details on computational costs). Detailed results per mode"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'title': '', 'subject': '', 'creationDate': 'D:20210604001958Z', 'total_pages': 22, 'creator': 'LaTeX with hyperref', 'trapped': '', 'creationdate': '2021-06-04T00:19:58+00:00', 'moddate': '2021-06-04T00:19:58+00:00', 'format': 'PDF 1.5', 'source': '../../rag_docs/vision_transformer.pdf', 'producer': 'pdfTeX-1.40.21', 'keywords': '', 'page': 0, 'modDate': 'D:20210604001958Z', 'file_path': '../../rag_docs/vision_transformer.pdf', 'author': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Published as a conference paper at ICLR 2021\n",
       "AN IMAGE IS WORTH 16X16 WORDS:\n",
       "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
       "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\n",
       "Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\n",
       "Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n",
       "∗equal technical contribution, †equal advising\n",
       "Google Research, Brain Team\n",
       "{adosovitskiy, neilhoulsby}@google.com\n",
       "ABSTRACT\n",
       "While the Transformer architecture has become the de-facto standard for natural\n",
       "language processing tasks, its applications to computer vision remain limited. In\n",
       "vision, attention is either applied in conjunction with convolutional networks, or\n",
       "used to replace certain components of convolutional networks while keeping their\n",
       "overall structure in place. We show that this reliance on CNNs is not necessary\n",
       "and a pure transformer applied directly to sequences of image patches can perform\n",
       "very well on image classiﬁcation tasks. When pre-traine"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'page': 1, 'producer': 'pdfTeX-1.40.21', 'source': '../../rag_docs/vision_transformer.pdf', 'trapped': '', 'subject': '', 'title': '', 'keywords': '', 'format': 'PDF 1.5', 'moddate': '2021-06-04T00:19:58+00:00', 'creationdate': '2021-06-04T00:19:58+00:00', 'file_path': '../../rag_docs/vision_transformer.pdf', 'creator': 'LaTeX with hyperref', 'creationDate': 'D:20210604001958Z', 'total_pages': 22, 'author': '', 'modDate': 'D:20210604001958Z'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Published as a conference paper at ICLR 2021\n",
       "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\n",
       "when trained on insufﬁcient amounts of data.\n",
       "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
       "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
       "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
       "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\n",
       "or beats state of the art on multiple image recognition benchmarks. In particular, the best model\n",
       "reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\n",
       "and 77.63% on the VTAB suite of 19 tasks.\n",
       "2\n",
       "RELATED WORK\n",
       "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\n",
       "come the state of the art method in many NLP tasks. Large Transformer-based mo"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'trapped': '', 'creator': 'LaTeX with hyperref', 'keywords': '', 'producer': 'pdfTeX-1.40.21', 'total_pages': 22, 'page': 4, 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../rag_docs/vision_transformer.pdf', 'modDate': 'D:20210604001958Z', 'title': '', 'file_path': '../../rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'moddate': '2021-06-04T00:19:58+00:00', 'subject': '', 'creationDate': 'D:20210604001958Z'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Published as a conference paper at ICLR 2021\n",
       "Model\n",
       "Layers\n",
       "Hidden size D\n",
       "MLP size\n",
       "Heads\n",
       "Params\n",
       "ViT-Base\n",
       "12\n",
       "768\n",
       "3072\n",
       "12\n",
       "86M\n",
       "ViT-Large\n",
       "24\n",
       "1024\n",
       "4096\n",
       "16\n",
       "307M\n",
       "ViT-Huge\n",
       "32\n",
       "1280\n",
       "5120\n",
       "16\n",
       "632M\n",
       "Table 1: Details of Vision Transformer model variants.\n",
       "We also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\n",
       "low-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\n",
       "three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\n",
       "imagery, and Structured – tasks that require geometric understanding like localization.\n",
       "Model Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\n",
       "summarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\n",
       "add the larger “Huge” model. In what follows we use brief notation to indicate the model size and\n",
       "the input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch siz"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'total_pages': 22, 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'author': '', 'creator': 'LaTeX with hyperref', 'trapped': '', 'subject': '', 'moddate': '2021-06-04T00:19:58+00:00', 'creationdate': '2021-06-04T00:19:58+00:00', 'keywords': '', 'file_path': '../../rag_docs/vision_transformer.pdf', 'title': '', 'modDate': 'D:20210604001958Z', 'format': 'PDF 1.5', 'page': 3, 'source': '../../rag_docs/vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Published as a conference paper at ICLR 2021\n",
       "The MLP contains two layers with a GELU non-linearity.\n",
       "z0 = [xclass; x1\n",
       "pE; x2\n",
       "pE; · · · ; xN\n",
       "p E] + Epos,\n",
       "E ∈R(P 2·C)×D, Epos ∈R(N+1)×D\n",
       "(1)\n",
       "z′\n",
       "ℓ= MSA(LN(zℓ−1)) + zℓ−1,\n",
       "ℓ= 1 . . . L\n",
       "(2)\n",
       "zℓ= MLP(LN(z′\n",
       "ℓ)) + z′\n",
       "ℓ,\n",
       "ℓ= 1 . . . L\n",
       "(3)\n",
       "y = LN(z0\n",
       "L)\n",
       "(4)\n",
       "Inductive bias.\n",
       "We note that Vision Transformer has much less image-speciﬁc inductive bias than\n",
       "CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\n",
       "baked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\n",
       "tionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\n",
       "structure is used very sparingly: in the beginning of the model by cutting the image into patches and\n",
       "at ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\n",
       "scribed below). Other than that, the position embeddings at initialization time carry no information\n",
       "about the 2D pos"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the difference between transformers and vision transformers?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "display_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQFWv7YUyVII"
   },
   "source": [
    "## Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt** refers to the input text or instructions provided to a language model (such as GPT) to guide its response. \n",
    "\n",
    "In Retrieval-Augmented Generation (RAG) systems, a prompt typically includes the user's question and relevant context retrieved from documents. The prompt is carefully designed to instruct the model on how to answer, what information to use, and how to format the output.\n",
    "\n",
    "For example, in this notebook, the `rag_prompt` variable below defines a template that combines the user's question and retrieved context to generate a well-structured answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PHOrfGXKyVIJ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG Prompt Template\n",
    "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
    "                Answer the following question using only the following pieces of retrieved context.\n",
    "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
    "                Keep the answer detailed and well formatted based on the information from the context.\n",
    "\n",
    "                Question:\n",
    "                {question}\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "\n",
    "# Create Chat Prompt Template provided by langchain ChatPromptTemplate\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "KmWeCB4yyVIJ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize ChatOpenAI model\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Function to format documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG Chain\n",
    "\n",
    "# The RAG chain first retrieves relevant documents based on the query,\n",
    "# formats them, and then uses the prompt template to generate an answer using the ChatGPT model\n",
    "# You can learn more about Langchain Runnables here: https://python.langchain.com/en/latest/modules/core/runnables.html\n",
    "q_and_a_rag_chain = (\n",
    "    {\n",
    "        \"context\": (retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough() # pass the question as is\n",
    "    }\n",
    "      |\n",
    "    rag_prompt_template # Use the RAG prompt template\n",
    "      |\n",
    "    chatgpt # Generate answer using ChatGPT model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "xvj_eGIWyVIJ",
    "outputId": "9f8c87a6-077c-4b11-de39-a387b7b0e161"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Machine learning is a subfield of computer science that provides computers with the ability to learn from data without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence (AI). Machine learning focuses on the study and construction of algorithms that can learn from and make predictions based on data. These algorithms follow programmed instructions but can also make decisions or predictions based on the data they process.\n",
       "\n",
       "In machine learning, algorithms build models from sample inputs, which allows them to perform tasks where traditional programming methods are insufficient. Common applications of machine learning include spam filtering, detecting network intrusions, optical character recognition (OCR), search engines, and computer vision.\n",
       "\n",
       "There are different types of machine learning, including:\n",
       "\n",
       "1. **Supervised Learning**: This involves inferring a function from labeled training data, where the results are known beforehand. The system learns to produce correct outputs based on the input data, typically using vectors to represent both the training data and the results.\n",
       "\n",
       "2. **Deep Learning**: A subset of machine learning that utilizes neural networks with multiple layers (known as multi-layer neural networks). Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition. It processes information in a hierarchical manner, with each layer extracting increasingly abstract features from the data.\n",
       "\n",
       "Overall, machine learning enables computers to adapt and improve their performance on tasks through experience, making it a powerful tool in the realm of artificial intelligence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "result = q_and_a_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "pXtezDlZzadt",
    "outputId": "fdec5afe-f61c-436e-a5b4-c87319222292"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A CNN, or Convolutional Neural Network, is a type of artificial neural network primarily used for image-driven pattern recognition tasks. CNNs are designed to process data with a grid-like topology, such as images, and they consist of three main types of layers: convolutional layers, pooling layers, and fully-connected layers.\n",
       "\n",
       "### Key Features of CNNs:\n",
       "\n",
       "1. **Architecture**: \n",
       "   - CNNs are structured to handle the spatial dimensionality of input data, which includes height, width, and depth (for color channels in images).\n",
       "   - The architecture typically involves stacking multiple convolutional layers followed by pooling layers, which helps in reducing the dimensionality of the data while retaining important features.\n",
       "\n",
       "2. **Convolutional Layers**:\n",
       "   - These layers apply a convolution operation to the input, which involves sliding a filter (or kernel) over the input data to produce feature maps. Each neuron in a convolutional layer is connected to a small region of the input, allowing the network to learn spatial hierarchies of features.\n",
       "\n",
       "3. **Pooling Layers**:\n",
       "   - Pooling layers are used to down-sample the feature maps, reducing their dimensionality and helping to make the representation more manageable. This also aids in making the model invariant to small translations in the input.\n",
       "\n",
       "4. **Fully-Connected Layers**:\n",
       "   - After several convolutional and pooling layers, the high-level reasoning in the neural network is done through fully-connected layers, where every neuron is connected to every neuron in the previous layer.\n",
       "\n",
       "5. **Activation Functions**:\n",
       "   - CNNs often use activation functions like the Rectified Linear Unit (ReLU) to introduce non-linearity into the model, which helps in learning complex patterns.\n",
       "\n",
       "6. **Efficiency**:\n",
       "   - CNNs are designed to be computationally efficient, especially when dealing with large images, by reducing the number of parameters through shared weights in convolutional layers.\n",
       "\n",
       "7. **Applications**:\n",
       "   - CNNs are widely used in various applications, including image classification, object detection, and image segmentation, due to their ability to automatically learn and extract features from images.\n",
       "\n",
       "In summary, CNNs are a powerful class of neural networks that excel in tasks involving image data, leveraging their unique architecture to efficiently learn from and process visual information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is a CNN?\"\n",
    "result = q_and_a_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "J5IQoBc0zlAr",
    "outputId": "4a3af9b5-f867-43c3-c1a6-f1cba4e66fbe"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on enabling computers to automatically understand and generate human languages. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages. The overarching goal of NLP is to facilitate seamless interaction between computers and humans through language, allowing for tasks such as automatic translation, sentiment analysis, and conversational agents.\n",
       "\n",
       "NLP is closely related to linguistics, which is the scientific study of language and its structure. Linguistics provides the foundational theories and frameworks that inform NLP techniques, as understanding the nuances of human language—such as syntax, semantics, and pragmatics—is essential for developing effective NLP applications. By leveraging insights from linguistics, NLP aims to create systems that can comprehend and produce language in a way that is meaningful and contextually appropriate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is NLP and its relation to linguistics?\"\n",
    "result = q_and_a_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "Ttz53mEy0J_D",
    "outputId": "bf628efb-07ee-4695-aa4f-2f036694fdf1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try out a new query which answer is not present in the documents\n",
    "query = \"What is LangChain?\"\n",
    "result = q_and_a_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we built a simple Retrieval-Augmented Generation (RAG) system using LangChain and OpenAI models. We demonstrated how to:\n",
    "\n",
    "- Load and process both JSON and PDF documents.\n",
    "- Chunk documents for efficient retrieval.\n",
    "- Generate embeddings using OpenAI's embedding models.\n",
    "- Store and retrieve document vectors using Chroma as a vector database.\n",
    "- Retrieve relevant context based on semantic similarity.\n",
    "- Construct a RAG pipeline that combines retrieval and generation to answer user queries using retrieved context.\n",
    "\n",
    "This workflow enables robust question-answering over custom document collections, ensuring responses are grounded in the provided data. You can further extend this system by experimenting with different chunking strategies, embedding models, or integrating additional data sources.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Retrieval_Augmented_Generation_from_Basic_to_Advance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
